{
    "feed":{
       "@xmlns":"http://www.w3.org/2005/Atom",
       "link":{
          "@href":"http://arxiv.org/api/query?search_query%3Dcat%3Acs.AI%26id_list%3D%26start%3D0%26max_results%3D2",
          "@rel":"self",
          "@type":"application/atom+xml"
       },
       "title":{
          "@type":"html",
          "#text":"ArXiv Query: search_query=cat:cs.AI&id_list=&start=0&max_results=2"
       },
       "id":"http://arxiv.org/api/sLG0txIUz7g/GKW7ibPhDY0NNSQ",
       "updated":"2023-05-14T00:00:00-04:00",
       "opensearch:totalResults":{
          "@xmlns:opensearch":"http://a9.com/-/spec/opensearch/1.1/",
          "#text":"62725"
       },
       "opensearch:startIndex":{
          "@xmlns:opensearch":"http://a9.com/-/spec/opensearch/1.1/",
          "#text":"0"
       },
       "opensearch:itemsPerPage":{
          "@xmlns:opensearch":"http://a9.com/-/spec/opensearch/1.1/",
          "#text":"2"
       },
       "entry":[
          {
             "id":"http://arxiv.org/abs/2305.07011v1",
             "updated":"2023-05-11T17:53:29Z",
             "published":"2023-05-11T17:53:29Z",
             "title":"Region-Aware Pretraining for Open-Vocabulary Object Detection with\n  Vision Transformers",
             "summary":"We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a\ncontrastive image-text pretraining recipe to bridge the gap between image-level\npretraining and open-vocabulary object detection. At the pretraining phase, we\npropose to randomly crop and resize regions of positional embeddings instead of\nusing the whole image positional embeddings. This better matches the use of\npositional embeddings at region-level in the detection finetuning phase. In\naddition, we replace the common softmax cross entropy loss in contrastive\nlearning with focal loss to better learn the informative yet difficult\nexamples. Finally, we leverage recent advances in novel object proposals to\nimprove open-vocabulary detection finetuning. We evaluate our full model on the\nLVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer.\nRO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best\nexisting approach by +5.8 points in addition to competitive zero-shot transfer\ndetection. Surprisingly, RO-ViT improves the image-level representation as well\nand achieves the state of the art on 9 out of 12 metrics on COCO and Flickr\nimage-text retrieval benchmarks, outperforming competitive approaches with\nlarger models.",
             "author":[
                {
                   "name":"Dahun Kim"
                },
                {
                   "name":"Anelia Angelova"
                },
                {
                   "name":"Weicheng Kuo"
                }
             ],
             "arxiv:comment":{
                "@xmlns:arxiv":"http://arxiv.org/schemas/atom",
                "#text":"CVPR 2023"
             },
             "link":[
                {
                   "@href":"http://arxiv.org/abs/2305.07011v1",
                   "@rel":"alternate",
                   "@type":"text/html"
                },
                {
                   "@title":"pdf",
                   "@href":"http://arxiv.org/pdf/2305.07011v1",
                   "@rel":"related",
                   "@type":"application/pdf"
                }
             ],
             "arxiv:primary_category":{
                "@xmlns:arxiv":"http://arxiv.org/schemas/atom",
                "@term":"cs.CV",
                "@scheme":"http://arxiv.org/schemas/atom"
             },
             "category":[
                {
                   "@term":"cs.CV",
                   "@scheme":"http://arxiv.org/schemas/atom"
                },
                {
                   "@term":"cs.AI",
                   "@scheme":"http://arxiv.org/schemas/atom"
                },
                {
                   "@term":"cs.CL",
                   "@scheme":"http://arxiv.org/schemas/atom"
                }
             ]
          },
          {
             "id":"http://arxiv.org/abs/2305.06993v1",
             "updated":"2023-05-11T17:29:47Z",
             "published":"2023-05-11T17:29:47Z",
             "title":"SMATCH++: Standardized and Extended Evaluation of Semantic Graphs",
             "summary":"The Smatch metric is a popular method for evaluating graph distances, as is\nnecessary, for instance, to assess the performance of semantic graph parsing\nsystems. However, we observe some issues in the metric that jeopardize\nmeaningful evaluation. E.g., opaque pre-processing choices can affect results,\nand current graph-alignment solvers do not provide us with upper-bounds.\nWithout upper-bounds, however, fair evaluation is not guaranteed. Furthermore,\nadaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity)\nare spread out, and lack a unifying framework.\n  For better inspection, we divide the metric into three modules:\npre-processing, alignment, and scoring. Examining each module, we specify its\ngoals and diagnose potential issues, for which we discuss and test mitigation\nstrategies. For pre-processing, we show how to fully conform to annotation\nguidelines that allow structurally deviating but valid graphs. For safer and\nenhanced alignment, we show the feasibility of optimal alignment in a standard\nevaluation setup, and develop a lossless graph compression method that shrinks\nthe search space and significantly increases efficiency. For improved scoring,\nwe propose standardized and extended metric calculation of fine-grained\nsub-graph meaning aspects. Our code is available at\nhttps://github.com/flipz357/smatchpp",
             "author":{
                "name":"Juri Opitz"
             },
             "arxiv:comment":{
                "@xmlns:arxiv":"http://arxiv.org/schemas/atom",
                "#text":"EACL 2023 findings, Code: https://github.com/flipz357/smatchpp"
             },
             "link":[
                {
                   "@href":"http://arxiv.org/abs/2305.06993v1",
                   "@rel":"alternate",
                   "@type":"text/html"
                },
                {
                   "@title":"pdf",
                   "@href":"http://arxiv.org/pdf/2305.06993v1",
                   "@rel":"related",
                   "@type":"application/pdf"
                }
             ],
             "arxiv:primary_category":{
                "@xmlns:arxiv":"http://arxiv.org/schemas/atom",
                "@term":"cs.CL",
                "@scheme":"http://arxiv.org/schemas/atom"
             },
             "category":[
                {
                   "@term":"cs.CL",
                   "@scheme":"http://arxiv.org/schemas/atom"
                },
                {
                   "@term":"cs.AI",
                   "@scheme":"http://arxiv.org/schemas/atom"
                }
             ]
          }
       ]
    }
 }